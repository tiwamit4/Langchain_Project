{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face x LangChain : A new partner package in LangChain\n",
    "langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_huggingface in d:\\langchain_project\\venv\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in d:\\langchain_project\\venv\\lib\\site-packages (from langchain_huggingface) (0.26.2)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in d:\\langchain_project\\venv\\lib\\site-packages (from langchain_huggingface) (0.3.15)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in d:\\langchain_project\\venv\\lib\\site-packages (from langchain_huggingface) (3.2.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in d:\\langchain_project\\venv\\lib\\site-packages (from langchain_huggingface) (0.20.2)\n",
      "Requirement already satisfied: transformers>=4.39.0 in d:\\langchain_project\\venv\\lib\\site-packages (from langchain_huggingface) (4.46.1)\n",
      "Requirement already satisfied: filelock in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\langchain_project\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in d:\\langchain_project\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.1.139)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in d:\\langchain_project\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\langchain_project\\venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (9.0.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\langchain_project\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\langchain_project\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.2)\n",
      "Requirement already satisfied: scipy in d:\\langchain_project\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.14.1)\n",
      "Requirement already satisfied: Pillow in d:\\langchain_project\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\langchain_project\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\langchain_project\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\langchain_project\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\langchain_project\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\langchain_project\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\langchain_project\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\langchain_project\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\langchain_project\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\langchain_project\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\langchain_project\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\langchain_project\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\langchain_project\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\langchain_project\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.8.30)\n",
      "Requirement already satisfied: networkx in d:\\langchain_project\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\langchain_project\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\langchain_project\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\langchain_project\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\langchain_project\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain_huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\langchain_project\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\langchain_project\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: anyio in d:\\langchain_project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\langchain_project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.6)\n",
      "Requirement already satisfied: sniffio in d:\\langchain_project\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\langchain_project\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\langchain_project\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.0.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\langchain_project\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in d:\\langchain_project\\venv\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface_hub) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface_hub) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\langchain_project\\venv\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\langchain_project\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\langchain_project\\venv\\lib\\site-packages (from requests->huggingface_hub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\langchain_project\\venv\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\langchain_project\\venv\\lib\\site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\langchain_project\\venv\\lib\\site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "## API Call\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpoint\n",
    "#### How to Access HuggingFace Models with API\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "d:\\Langchain_project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\nMachine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.\\n\\nThe process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust actions accordingly.\\n\\nMachine learning is a popular topic in the field of data mining, which is the process of extracting useful patterns from large amounts of data. Data mining involves analyzing data from various perspectives and summarizing it into useful information for decision making.\\n\\nThe data-driven approach to machine learning makes it possible to build applications that can learn from experience and predict outcomes within a reasonable margin of error.\\n\\nThe key benefits of machine learning include:\\n\\n* Automating repetitive tasks\\n* Making accurate predictions and decisions\\n* Improving productivity and efficiency\\n* Reducing human error\\n* Enhancing customer experience\\n* Increasing profitability\\n\\nThere are three main types of machine learning:\\n\\n1. Supervised Learning: In supervised learning, the machine is provided with a labeled dataset, and it learns to make predictions based on the input-output pairs. The machine is trained to learn the relationship between the input variables and the output variable. The goal is to learn a function that maps inputs to outputs.\\n\\n2. Unsupervised Learning: In unsupervised learning, the machine is not provided with a labeled dataset. Instead, it learns to identify patterns and relationships in the data on its own. The goal is to learn a structure or a representation of the data that can be used to make meaningful inferences.\\n\\n3. Reinforcement Learning: In reinforcement learning, the machine learns to make decisions by interacting with an environment. The machine learns to take actions that maximize a reward signal. The goal is to learn a policy that maps states to actions.\\n\\nMachine learning is used in various applications such as image recognition, speech recognition, natural language processing, recommendation systems, fraud detection, and autonomous vehicles. It is a powerful tool that can help businesses and organizations to make better decisions, improve efficiency, and increase profitability.\\n\\nIn conclusion,'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🤖 and how can it benefit your business? Generative AI is a type of artificial intelligence that can create new content, such as text, images, and music, based on patterns it has learned from data. It differs from other types of AI, such as discriminative AI, which can only make predictions or classify existing data.\\n\\nOne of the key benefits of generative AI for businesses is its ability to automate content creation. This can save time and resources, especially for businesses that produce a large amount of content on a regular basis. For example, a company might use generative AI to write product descriptions, blog posts, or social media updates.\\n\\nAnother benefit of generative AI is its ability to generate new, creative ideas. By analyzing large amounts of data and identifying patterns, generative AI can come up with new combinations and ideas that humans might not have thought of. This can be especially useful for businesses in creative industries, such as advertising, marketing, and design.\\n\\nGenerative AI can also be used for more practical applications, such as generating new designs for products or predicting future trends. For example, a company might use generative AI to design new sneakers based on trends in fashion or to predict which colors and styles will be popular in the coming year.\\n\\nOverall, generative AI has the potential to greatly benefit businesses by automating content creation, generating new ideas, and improving productivity. However, it is important to note that generative AI is still a relatively new and rapidly evolving field, and there are still many challenges to overcome before it can be widely adopted. It is also important to ensure that generative AI is used ethically and responsibly, and that it does not infringe on the rights of individuals or violate privacy laws.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is generative AI \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/gemma-2-9b', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"google/gemma-2-9b\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: 1SIWdwjs8qpGnOcwUFd0k)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nMake sure your token has the correct permissions.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Langchain_project\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Langchain_project\\venv\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/google/gemma-2-9b",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is machine learning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Langchain_project\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:390\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    387\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    388\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    391\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    392\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    393\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    394\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    395\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    396\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    397\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    398\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    399\u001b[0m         )\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    402\u001b[0m     )\n",
      "File \u001b[1;32md:\\Langchain_project\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:755\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    749\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    753\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    754\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Langchain_project\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:950\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    936\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    937\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    938\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    948\u001b[0m         )\n\u001b[0;32m    949\u001b[0m     ]\n\u001b[1;32m--> 950\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    951\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    952\u001b[0m     )\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Langchain_project\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    791\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    793\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32md:\\Langchain_project\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:779\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    771\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    776\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    778\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 779\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    780\u001b[0m                 prompts,\n\u001b[0;32m    781\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    782\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    783\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    784\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    785\u001b[0m             )\n\u001b[0;32m    786\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    787\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    788\u001b[0m         )\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Langchain_project\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1502\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1501\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1502\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1503\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1504\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1505\u001b[0m     )\n\u001b[0;32m   1506\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32md:\\Langchain_project\\venv\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:312\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    311\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 312\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[1;32md:\\Langchain_project\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:296\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 296\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32md:\\Langchain_project\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:468\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    467\u001b[0m     )\n\u001b[1;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[0;32m    471\u001b[0m     range_header \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: (Request ID: 1SIWdwjs8qpGnOcwUFd0k)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nMake sure your token has the correct permissions.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."
     ]
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={} template='\\nQuestion:{question}\\nAnswer:Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "template=\"\"\"\n",
    "Question:{question}\n",
    "Answer:Lets think step by step.\n",
    "\"\"\"\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiwar\\AppData\\Local\\Temp\\ipykernel_19552\\3735178259.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain=LLMChain(llm=llm,prompt=prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"? India won the cricket World Cup 2011, defeating Sri Lanka in the final match held at Wankhede Stadium in Mumbai, India on April 2, 2011. India's victory marked the first time they had won the World Cup on home soil.\\n\\nMS Dhoni, the captain of the Indian team, was named the Man of the Match for his unbeaten 91 runs off 79 balls, including a crucial six in the final over of the match that helped India secure a six-wicket victory. This victory also marked the first time that India had won back-to-back ICC events, having won the 2010 ICC World Twenty20 earlier in the year.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke(\"Who won the cricket World up 2011\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Langchain_project\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tiwar\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = hf.embed_query(\"hi this is harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.02841656468808651,\n",
       " 0.012183268554508686,\n",
       " 0.027443984523415565,\n",
       " -0.05482865497469902,\n",
       " 0.024238867685198784,\n",
       " 0.0007662881398573518,\n",
       " 0.06783364713191986,\n",
       " 0.016348354518413544,\n",
       " -0.018950767815113068,\n",
       " 0.012542914599180222,\n",
       " 0.021564999595284462,\n",
       " -0.08793038874864578,\n",
       " 0.0006460413569584489,\n",
       " 0.03327082097530365,\n",
       " 0.005463749170303345,\n",
       " -0.06037645414471626,\n",
       " 0.05042261630296707,\n",
       " 0.004434795584529638,\n",
       " 0.0009598496835678816,\n",
       " 0.0017405670369043946,\n",
       " 0.003298831405118108,\n",
       " 0.03167250379920006,\n",
       " -0.04880749061703682,\n",
       " -0.044819142669439316,\n",
       " 0.07132111489772797,\n",
       " -0.007510872557759285,\n",
       " -0.0011259716702625155,\n",
       " -0.015801170840859413,\n",
       " -0.029402365908026695,\n",
       " -0.17224565148353577,\n",
       " -0.03189518675208092,\n",
       " -0.001629168982617557,\n",
       " 0.018104983493685722,\n",
       " 0.0153154032304883,\n",
       " -0.020729567855596542,\n",
       " -0.008872982114553452,\n",
       " -0.0012822651769965887,\n",
       " 0.027276871725916862,\n",
       " -0.010114259086549282,\n",
       " 0.012621616013348103,\n",
       " -0.007077881135046482,\n",
       " -0.016693193465471268,\n",
       " 0.040855828672647476,\n",
       " 0.023938357830047607,\n",
       " -0.0200815387070179,\n",
       " 0.028681132942438126,\n",
       " -0.019400736317038536,\n",
       " -0.014618180692195892,\n",
       " 0.0173796359449625,\n",
       " 0.0041640931740403175,\n",
       " 0.06415650993585587,\n",
       " 0.04768305644392967,\n",
       " 0.0018365010619163513,\n",
       " -8.072093623923138e-05,\n",
       " 0.016596803441643715,\n",
       " 0.011124194599688053,\n",
       " 0.0696943998336792,\n",
       " 0.051820479333400726,\n",
       " 0.05568530410528183,\n",
       " 0.05551542714238167,\n",
       " 0.0005039449315518141,\n",
       " 0.0418706014752388,\n",
       " -0.15344089269638062,\n",
       " 0.05180782824754715,\n",
       " 0.006689794361591339,\n",
       " -0.031670715659856796,\n",
       " -0.00910500343888998,\n",
       " -0.05160471796989441,\n",
       " 0.042508576065301895,\n",
       " 0.028200050815939903,\n",
       " -0.010748136788606644,\n",
       " 0.022405795753002167,\n",
       " 0.04439550265669823,\n",
       " 0.004115518182516098,\n",
       " 0.01899847388267517,\n",
       " -0.004357174038887024,\n",
       " 0.04762765020132065,\n",
       " 0.011824645102024078,\n",
       " 0.008164611645042896,\n",
       " 0.008177300915122032,\n",
       " -0.009698744863271713,\n",
       " -0.01426029484719038,\n",
       " 0.011409717611968517,\n",
       " -0.07362115383148193,\n",
       " -0.054395221173763275,\n",
       " -0.05703963339328766,\n",
       " -0.0036085746251046658,\n",
       " 0.0026660864241421223,\n",
       " 0.023782452568411827,\n",
       " 0.01537621021270752,\n",
       " -0.07020371407270432,\n",
       " -0.03130034729838371,\n",
       " -0.0031142805237323046,\n",
       " -0.015812169760465622,\n",
       " -0.037913978099823,\n",
       " -0.02592192403972149,\n",
       " 0.018168406561017036,\n",
       " -0.03882461413741112,\n",
       " -0.056745074689388275,\n",
       " 0.5792059898376465,\n",
       " -0.05278834328055382,\n",
       " 0.020716384053230286,\n",
       " 0.06794390827417374,\n",
       " -0.04541653394699097,\n",
       " 0.011642461642622948,\n",
       " -0.02157175913453102,\n",
       " 0.020341727882623672,\n",
       " -0.027448948472738266,\n",
       " -0.04558895155787468,\n",
       " -0.02944357879459858,\n",
       " -0.023662520572543144,\n",
       " -0.03431525453925133,\n",
       " 0.0019388606306165457,\n",
       " -0.07095140218734741,\n",
       " 0.034556325525045395,\n",
       " -0.030558940023183823,\n",
       " 0.03907861188054085,\n",
       " -0.029707303270697594,\n",
       " -0.0008282861672341824,\n",
       " -0.012159358710050583,\n",
       " -0.018272804096341133,\n",
       " 0.025486506521701813,\n",
       " -0.004461637232452631,\n",
       " 0.016335321590304375,\n",
       " 0.01912650652229786,\n",
       " -0.05483203008770943,\n",
       " 0.027635984122753143,\n",
       " -0.004757650196552277,\n",
       " 0.05900171399116516,\n",
       " -0.0016944739036262035,\n",
       " 0.008015000261366367,\n",
       " -0.037726834416389465,\n",
       " -0.09893040359020233,\n",
       " -0.022574398666620255,\n",
       " -0.037604689598083496,\n",
       " -0.002169860526919365,\n",
       " 0.0032446151599287987,\n",
       " -0.019202526658773422,\n",
       " -0.00863122008740902,\n",
       " -0.04802309721708298,\n",
       " 0.00869671069085598,\n",
       " -0.09516111761331558,\n",
       " -0.03496047854423523,\n",
       " -0.04360799118876457,\n",
       " -0.0003440282307565212,\n",
       " -0.010173655115067959,\n",
       " -0.030999546870589256,\n",
       " 0.024309691041707993,\n",
       " -0.02040202170610428,\n",
       " 0.031139401718974113,\n",
       " 0.0008811111329123378,\n",
       " 0.013916490599513054,\n",
       " -0.0311962328851223,\n",
       " -0.037154048681259155,\n",
       " 0.004029621835798025,\n",
       " 0.014799792319536209,\n",
       " 0.04318893328309059,\n",
       " 0.03875483572483063,\n",
       " 0.013852010481059551,\n",
       " 0.019797861576080322,\n",
       " 0.010267049074172974,\n",
       " -0.005434098187834024,\n",
       " -0.014299226924777031,\n",
       " 0.027637822553515434,\n",
       " 0.009802641347050667,\n",
       " -0.13550284504890442,\n",
       " -0.017139768227934837,\n",
       " 0.017617113888263702,\n",
       " 0.023132257163524628,\n",
       " 0.0017590099014341831,\n",
       " 0.030889414250850677,\n",
       " 0.0399186909198761,\n",
       " -0.013684151694178581,\n",
       " 0.024816518649458885,\n",
       " 0.05405019223690033,\n",
       " 0.017761152237653732,\n",
       " -0.018475066870450974,\n",
       " 0.02595539763569832,\n",
       " -0.0063775149174034595,\n",
       " -0.01658732257783413,\n",
       " 0.037848006933927536,\n",
       " -0.027290036901831627,\n",
       " -0.0528457909822464,\n",
       " -0.038033176213502884,\n",
       " 0.05191105976700783,\n",
       " -0.00755709782242775,\n",
       " -0.03180527687072754,\n",
       " 0.013284189626574516,\n",
       " -0.027723705396056175,\n",
       " 0.05630652233958244,\n",
       " 0.0030418927781283855,\n",
       " 0.05332484096288681,\n",
       " -0.0579112246632576,\n",
       " -0.011325825937092304,\n",
       " -0.031171994283795357,\n",
       " 0.02560870349407196,\n",
       " 0.033890608698129654,\n",
       " -0.0010284590534865856,\n",
       " 0.01586488075554371,\n",
       " 0.01059521920979023,\n",
       " -0.027037804946303368,\n",
       " -0.0009308407315984368,\n",
       " -0.04815221577882767,\n",
       " 0.028179233893752098,\n",
       " 0.010320608504116535,\n",
       " 0.06662959605455399,\n",
       " -0.01655818335711956,\n",
       " -0.004431380890309811,\n",
       " 0.03823424503207207,\n",
       " -0.023408209905028343,\n",
       " -0.035581737756729126,\n",
       " -0.05829070881009102,\n",
       " -0.011181486770510674,\n",
       " -0.017684556543827057,\n",
       " -0.016141297295689583,\n",
       " -0.03424534946680069,\n",
       " -0.025139542296528816,\n",
       " 0.03939666599035263,\n",
       " -0.023658229038119316,\n",
       " -0.007725008763372898,\n",
       " -0.005098927300423384,\n",
       " -0.03523437678813934,\n",
       " -0.014076863415539265,\n",
       " -0.2232602834701538,\n",
       " -0.031471364200115204,\n",
       " -0.0012906071497127414,\n",
       " -0.0017199907451868057,\n",
       " -0.007846049033105373,\n",
       " -0.058023251593112946,\n",
       " 0.046174563467502594,\n",
       " 0.02455265074968338,\n",
       " 0.07320840656757355,\n",
       " 0.017268355935811996,\n",
       " 0.047612063586711884,\n",
       " 0.01347330305725336,\n",
       " -0.005516010336577892,\n",
       " -0.014357834123075008,\n",
       " -0.009674319997429848,\n",
       " 0.04878249391913414,\n",
       " 0.03053811378777027,\n",
       " -0.024993976578116417,\n",
       " 0.021486258134245872,\n",
       " 0.017639806494116783,\n",
       " 0.053138937801122665,\n",
       " 0.013484999537467957,\n",
       " -0.023226004093885422,\n",
       " -0.02140398696064949,\n",
       " 0.026075372472405434,\n",
       " 0.002029179595410824,\n",
       " 0.12753744423389435,\n",
       " 0.08316835761070251,\n",
       " 0.04408949986100197,\n",
       " -0.026703620329499245,\n",
       " 0.005522000603377819,\n",
       " -0.009294911287724972,\n",
       " 0.020074333995580673,\n",
       " -0.09684176743030548,\n",
       " -0.024703925475478172,\n",
       " 0.02508697845041752,\n",
       " 0.002088631736114621,\n",
       " -0.044894028455019,\n",
       " -0.0786113515496254,\n",
       " -0.004376342985779047,\n",
       " -0.06590455025434494,\n",
       " 0.014689408242702484,\n",
       " -0.057641852647066116,\n",
       " -0.07152028381824493,\n",
       " -0.06232648715376854,\n",
       " 0.0034316396340727806,\n",
       " -0.046065498143434525,\n",
       " 0.045300863683223724,\n",
       " -0.026762252673506737,\n",
       " 0.034010931849479675,\n",
       " 0.04547380656003952,\n",
       " -0.028179218992590904,\n",
       " 0.005011801607906818,\n",
       " 0.009630811400711536,\n",
       " -0.030305616557598114,\n",
       " -0.036124806851148605,\n",
       " -0.013626936823129654,\n",
       " -0.032653722912073135,\n",
       " -0.04467751830816269,\n",
       " 0.010642170906066895,\n",
       " -0.027486328035593033,\n",
       " -0.024565106257796288,\n",
       " -0.024747760966420174,\n",
       " 0.05361957848072052,\n",
       " 0.020789971575140953,\n",
       " 0.019468478858470917,\n",
       " 0.053241219371557236,\n",
       " -0.014002474024891853,\n",
       " 0.021243248134851456,\n",
       " -0.049573253840208054,\n",
       " -0.008522587828338146,\n",
       " 0.007852853275835514,\n",
       " -0.05719394236803055,\n",
       " -0.027550632134079933,\n",
       " 0.005300886929035187,\n",
       " 0.040072910487651825,\n",
       " 0.019597908481955528,\n",
       " -0.04519734904170036,\n",
       " 0.0324358195066452,\n",
       " -0.012342404574155807,\n",
       " 0.034314416348934174,\n",
       " 0.02110210247337818,\n",
       " 0.03984646871685982,\n",
       " 0.03166377171874046,\n",
       " -0.03359021991491318,\n",
       " 0.03164784982800484,\n",
       " -0.0033045082818716764,\n",
       " 0.004641848616302013,\n",
       " 0.03758932277560234,\n",
       " -0.05924459919333458,\n",
       " 0.007028403226286173,\n",
       " 0.003808701876550913,\n",
       " -0.02578888274729252,\n",
       " -0.02120332419872284,\n",
       " 0.022691210731863976,\n",
       " -0.021772993728518486,\n",
       " -0.27963775396347046,\n",
       " 0.007267419248819351,\n",
       " 0.0210720207542181,\n",
       " 0.045197442173957825,\n",
       " -0.02053447626531124,\n",
       " 0.024313705042004585,\n",
       " -0.0006137107848189771,\n",
       " -0.01185702532529831,\n",
       " -0.032967787235975266,\n",
       " 0.0358431376516819,\n",
       " 0.0312817357480526,\n",
       " 0.06373955309391022,\n",
       " 0.04654783755540848,\n",
       " -0.014470523223280907,\n",
       " 0.01586974412202835,\n",
       " 0.03397127613425255,\n",
       " 0.018059568479657173,\n",
       " 0.002298766979947686,\n",
       " 0.0165498536080122,\n",
       " -0.02171487547457218,\n",
       " -0.03485997021198273,\n",
       " -0.0008649277733638883,\n",
       " 0.15126042068004608,\n",
       " -0.024536756798624992,\n",
       " 0.030671212822198868,\n",
       " -0.007318181451410055,\n",
       " -0.006135466508567333,\n",
       " 0.06415144354104996,\n",
       " 0.016021493822336197,\n",
       " -0.03636444732546806,\n",
       " 0.01989860273897648,\n",
       " -0.021172314882278442,\n",
       " 0.04829411953687668,\n",
       " -0.044780977070331573,\n",
       " 0.047633808106184006,\n",
       " 0.0007749417563900352,\n",
       " -0.0059279585257172585,\n",
       " 0.061542678624391556,\n",
       " 0.023968392983078957,\n",
       " 0.01330502424389124,\n",
       " 0.022684510797262192,\n",
       " 0.014538082294166088,\n",
       " -0.052159082144498825,\n",
       " -0.032749660313129425,\n",
       " 0.08583346009254456,\n",
       " -0.003724851878359914,\n",
       " 0.0013493997976183891,\n",
       " 0.040919914841651917,\n",
       " 0.011659649200737476,\n",
       " 0.05843618884682655,\n",
       " -0.022286174818873405,\n",
       " -0.011520706117153168,\n",
       " 0.004705706145614386,\n",
       " 0.047182608395814896,\n",
       " -0.0019179129740223289,\n",
       " 0.033009376376867294,\n",
       " -0.03505055233836174,\n",
       " -0.020736567676067352,\n",
       " -0.009222187101840973,\n",
       " 0.01461828127503395,\n",
       " 0.006456071976572275,\n",
       " 0.0010978570207953453,\n",
       " 0.010223980993032455,\n",
       " 0.0853721871972084,\n",
       " 0.038839519023895264]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
